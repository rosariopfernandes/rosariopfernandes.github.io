---
layout: post
title:  "Explorando o ML Kit for Firebase"
date:   2018-05-09 05:00:00 +0200
categories: firebase
---
O artigo original pode ser encontrado no [Medium](https://medium.com/android-dev-moz/mlkit-540f8e5438c1).
<section name="298b" class="section section--body section--first"><h4 name="6983" id="6983" class="graf graf--h4 graf-after--h3 graf--subtitle">Parte 1 - O novo serviço de Machine Learning do Firebase</h4><p name="c42c" id="c42c" class="graf graf--p graf-after--h4">Este artigo faz parte da série Explorando o ML Kit for Firebase:</p><ol class="postList"><li name="e33c" id="e33c" class="graf graf--li graf-after--p">O novo serviço de Machine Learning do Firebase</li><li name="c66f" id="c66f" class="graf graf--li graf-after--li graf--trailing"><a href="/firebase/2019/02/01/Explorando-MLKit-Firebase2.html" data-href="/firebase/2019/02/01/Explorando-MLKit-Firebase2.html" class="markup--anchor markup--li-anchor" target="_blank">Linguagem Natural e Identificação de Linguagem</a></li></ol></section><section name="3bbb" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="881d" id="881d" class="graf graf--p graf--leading">Durante a sessão de abertura do <a href="https://events.google.com/io" data-href="https://events.google.com/io" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Google I/O 2018</a>, que aconteceu ontem(8 de Maio), a Google apresentou algumas novidades para o Firebase: melhorias no <a href="https://firebase.google.com/docs/ab-testing/" data-href="https://firebase.google.com/docs/ab-testing/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">A/B Testing</a>, suporte para <a href="https://firebase.google.com/docs/test-lab/ios/firebase-console" data-href="https://firebase.google.com/docs/test-lab/ios/firebase-console" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">iOS no Test Lab</a>, a versão estável do <a href="https://firebase.google.com/docs/perf-mon/" data-href="https://firebase.google.com/docs/perf-mon/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Performance Monitor</a> e a versão <em class="markup--em markup--p-em">preview</em> do <a href="https://firebase.google.com/docs/ml-kit/" data-href="https://firebase.google.com/docs/ml-kit/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ML Kit for Firebase</a>.</p><p name="b57a" id="b57a" class="graf graf--p graf-after--p">De acordo com a documentação, o <a href="https://firebase.google.com/products/ml-kit/" data-href="https://firebase.google.com/products/ml-kit/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ML Kit</a> for Firebase é:</p><blockquote name="bebb" id="bebb" class="graf graf--blockquote graf-after--p">um SDK para dispositivos móveis que leva Machine Learning da Google para aplicativos Android e iOS em um pacote poderoso e fácil de usar.</blockquote><p name="80f5" id="80f5" class="graf graf--p graf-after--blockquote">Você provavelmente já esteve numa situação em que precisava de implementar alguma forma de Inteligência Artificial ou Machine Learning na sua aplicação para tornar ela mais “inteligente” e melhorar a experiência do seu utilizador. Nessa situação, você provavelmente não conseguiu começar a utilizar Machine Learning por ser complicado de aprender, ligeiramente mais caro ou talvez porque você não queria hospedar um modelo ML em um outro serviço.<br>O ML Kit foi introduzido para resolver este problema. Com o ML Kit, você pode utilizar serviços de Machine Learning com a mesma simplicidade do Firebase.</p><p name="1ff0" id="1ff0" class="graf graf--p graf-after--p">ML Kit for Firebase traz-nos funcionalidades como:</p><ul class="postList"><li name="5bfe" id="5bfe" class="graf graf--li graf-after--p">Legendas para imagens;</li><li name="cf0d" id="cf0d" class="graf graf--li graf-after--li">Reconhecimento de texto;</li><li name="4a1b" id="4a1b" class="graf graf--li graf-after--li">Deteção facial;</li><li name="7aa1" id="7aa1" class="graf graf--li graf-after--li">Scan de código de barras;</li><li name="246a" id="246a" class="graf graf--li graf-after--li">Deteção de pontos de referência;</li><li name="1866" id="1866" class="graf graf--li graf-after--li">E respostas inteligentes? 🤔</li></ul><p name="ab92" id="ab92" class="graf graf--p graf-after--li">Fiquei bastante entusiasmado quando eles apresentaram esta funcionalidade que combina <a href="https://cloud.google.com/vision/?authuser=0" data-href="https://cloud.google.com/vision/?authuser=0" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Google Cloud Vision API</a>, <a href="https://www.tensorflow.org/mobile/tflite/?authuser=0" data-href="https://www.tensorflow.org/mobile/tflite/?authuser=0" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">TensorFlow Lite</a>, e <a href="https://developer.android.com/ndk/guides/neuralnetworks/?authuser=0" data-href="https://developer.android.com/ndk/guides/neuralnetworks/?authuser=0" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Android Neural Networks API</a>. Por isso, decidi experimentar na hora e partilhar a minha experiência.</p><h3 name="68ab" id="68ab" class="graf graf--h3 graf-after--p">On-device vs. Cloud</h3><p name="22a4" id="22a4" class="graf graf--p graf-after--h3">Antes de começar a falar sobre as tecnologias, gostaria de explicar a diferença entre on-device e cloud.</p><ul class="postList"><li name="192b" id="192b" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">on-device</strong> significa que a funcionalidade está disponível no dispositivo, e não precisa de uma conexão à internet para funcionar. No ML Kit, estas funcionalidades provavelmente são fornecidas através do TensorFlow Lite e Android Neural Networks API.<br>As tecnologias on-device do ML Kit são: Reconhecimento de Texto, deteção facial, scan de código de barras e legendas para imagens.</li><li name="7df1" id="7df1" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">cloud</strong> significa que a funcionalidade só está disponível na cloud. O que significa que o seu dispositivo móvel fará uma requisição à cloud e tem de esperar por uma resposta (através da internet, obviamente). O ML Kit utiliza a Cloud Vision API para estas funcionalidades. É importante notar que esta API precisa que você registre uma forma de pagamento para poder usá-la. No Firebase, isto significa fazer upgrade para o plano Blaze.<br>As tecnologias cloud do ML Kit são: Reconhecimento de Texto, Legendas para imagens e Detecção de pontos de referência.</li></ul><p name="758b" id="758b" class="graf graf--p graf-after--li">Se você estiver no plano Sparkle do Firebase, você só tem acesso às funcionalidades on-device do ML Kit, como mostra a <a href="https://firebase.google.com/pricing/?hl=pt-pt" data-href="https://firebase.google.com/pricing/?hl=pt-pt" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">tabela de preços do Firebase</a>. Para utilizar as funcionalidades cloud, você terá de passar para o plano Blaze, como mencionei anteriormente.<br>Neste artigo irei demonstrar apenas as funcionalidades on-device.</p><p name="b2d6" id="b2d6" class="graf graf--p graf-after--p graf--trailing">Vou lembrar-lhe que esta funcionalidade ainda está na versão <em class="markup--em markup--p-em">“preview”</em>, o que significa que só pode ser utilizada para testes. <strong class="markup--strong markup--p-strong">Não utilize em aplicações que se encontram em produção.</strong></p></div></div></section><section name="7bdc" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="65d6" id="65d6" class="graf graf--h3 graf--leading">ML Vision</h3><p name="7b86" id="7b86" class="graf graf--p graf-after--h3">Vision é o novo SDK do Firebase que permite que você use os pacotes de inteligência artificial que fazem o uso da câmera do dispositivo. Todas as funcionalidades que mencionei acima estão inclusas neste pacote.</p><p name="07d8" id="07d8" class="graf graf--p graf-after--p">Para poder ter estas funcionalidades na sua app, você precisa de:</p><p name="981f" id="981f" class="graf graf--p graf-after--p">1. <a href="https://firebase.google.com/docs/android/setup?hl=pt-pt" data-href="https://firebase.google.com/docs/android/setup?hl=pt-pt" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Adicionar o Firebase à app</a> e de seguida adicionar a dependência do ML Vision:</p><pre name="2b4d" id="2b4d" class="graf graf--pre graf-after--p">implementation &#39;com.google.firebase:firebase-ml-vision:15.0.0&#39;</pre><p name="8b99" id="8b99" class="graf graf--p graf-after--pre">2. (opcional) Adicionar o modelo ML à sua app. Basta colocar a seguinte linha ao AndroidManifest.xml:</p><pre name="315a" id="315a" class="graf graf--pre graf-after--p">&lt;meta-data<br>    android:name=&quot;com.google.firebase.ml.vision.DEPENDENCIES&quot;<br>    android:value=&quot;text&quot; /&gt;</pre><p name="4c8d" id="4c8d" class="graf graf--p graf-after--pre">Isto fará com que a app baixe o modelo ML quando for instalada pela Google Playstore.</p><p name="6c3d" id="6c3d" class="graf graf--p graf-after--p">O valor “text” serve para a funcionalidade de reconhecimento de texto. Se você quiser adicionar outra funcionalidade, pode colocá-las separadas por vírgula, por exemplo:</p><pre name="531f" id="531f" class="graf graf--pre graf-after--p">android:value=&quot;text,barcode,face,label&quot;</pre><p name="78c4" id="78c4" class="graf graf--p graf-after--pre">3. Criar uma variável do tipo <code class="markup--code markup--p-code">FirebaseVisionImage</code>. Para instanciá-la, você precisa passar por parâmetro um <code class="markup--code markup--p-code">Bitmap</code>, <code class="markup--code markup--p-code">Image</code>, <code class="markup--code markup--p-code">ByteBuffer</code> (ou array de <code class="markup--code markup--p-code">byte</code>) ou um <code class="markup--code markup--p-code">Uri</code> que contem o caminho da imagem.<br>Note que independentemente do parâmetro que você passa, é importante que a imagem esteja “de pé”, ou seja na rotação correta, senão o Firebase não consegue reconhece-la.</p><p name="cc74" id="cc74" class="graf graf--p graf-after--p">Para experimentar o ML Kit eu instanciei a classe <code class="markup--code markup--p-code">FirebaseVisionImage</code> com uma <code class="markup--code markup--p-code">Uri</code>. Estou a utilizar imagens da galeria por ser a forma mais simples de usar o ML Kit.</p><h4 name="5a14" id="5a14" class="graf graf--h4 graf-after--p">Reconhecimento de Texto</h4><p name="f371" id="f371" class="graf graf--p graf-after--h4">Esta funcionalidade permite que o dispositivo do utilizador leia texto (em um idioma baseado no latim) em tempo real, através da câmara.</p><p name="55c5" id="55c5" class="graf graf--p graf-after--p">Para utilizar o Reconhecimento de Texto, você precisa de utilizar o <code class="markup--code markup--p-code">FirebaseVisionTextDetector</code>:</p><pre name="477e" id="477e" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">FirebaseVisionTextDetector detector = FirebaseVision.getInstance()<br>    .getVisionTextDetector();</code></pre><p name="709a" id="709a" class="graf graf--p graf-after--pre">De seguida, chame o método <code class="markup--code markup--p-code">detectInImage()</code>, passando a nossa FirebaseVisionImage e adicione um listener. Nesse listener você poderá obter os blocos de texto que foram lidos da imagem. Veja o código completo no fim do artigo.</p><h4 name="d2de" id="d2de" class="graf graf--h4 graf-after--p">Scan de código de barras</h4><p name="12e0" id="12e0" class="graf graf--p graf-after--h4">Tal como o nome indica, esta funcionalidade permite ler um código de barras. Para tal, você precisa especificar os formatos de código de barras que a sua aplicação pode ler(veja a <a href="https://firebase.google.com/docs/ml-kit/android/read-barcodes?hl=pt-pt#configure-the-barcode-detector" data-href="https://firebase.google.com/docs/ml-kit/android/read-barcodes?hl=pt-pt#configure-the-barcode-detector" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">lista completa de formatos aqui</a>), através de um objecto <code class="markup--code markup--p-code">FirebaseVisionBarcodeDetectorOptions</code>:</p><pre name="d474" id="d474" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">FirebaseVisionBarcodeDetectorOptions options =<br>    new FirebaseVisionBarcodeDetectorOptions.Builder()<br>        .setBarcodeFormats(FirebaseVisionBarcode.FORMAT_QR_CODE,<br>                           FirebaseVisionBarcode.FORMAT_AZTEC)<br>        .build();</code></pre><p name="e9fe" id="e9fe" class="graf graf--p graf-after--pre">De seguida, o processo é o mesmo: usar o Detector, adicionar um listener e receber o código de barras.</p><pre name="5383" id="5383" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">FirebaseVisionBarcodeDetector detector = FirebaseVision.getInstance()<br>    .getVisionBarcodeDetector(options);</code></pre><p name="0c5a" id="0c5a" class="graf graf--p graf-after--pre">Encontre o código completo no fim do artigo.</p><h4 name="cfe9" id="cfe9" class="graf graf--h4 graf-after--p">Deteção Facial</h4><p name="2703" id="2703" class="graf graf--p graf-after--h4">A deteção facial é capaz de encontrar faces humanas numa imagem e identificar características como:</p><ul class="postList"><li name="33ed" id="33ed" class="graf graf--li graf-after--p">ângulo da face;</li><li name="5a20" id="5a20" class="graf graf--li graf-after--li">pontos de interesse — olhos(o olho esquerdo é distinguido do direito), boca, nariz, etc.</li><li name="3803" id="3803" class="graf graf--li graf-after--li">características — deteta se a pessoa está a sorrir, está com os olhos abertos, etc.</li></ul><p name="4503" id="4503" class="graf graf--p graf-after--li">E você já conhece o processo:</p><pre name="3b69" id="3b69" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">FirebaseVisionFaceDetector detector = FirebaseVision.getInstance()<br>    .getVisionFaceDetector();</code></pre><p name="5eb0" id="5eb0" class="graf graf--p graf-after--pre">Para esta experiência eu utilizei os valores que vêm definidos no exemplo dado na documentação do Firebase. Mas provavelmente irei escrever um artigo com mais detalhes sobre a deteção facial.</p><h4 name="8969" id="8969" class="graf graf--h4 graf-after--p">Legendas para imagens</h4><p name="1a02" id="1a02" class="graf graf--p graf-after--h4">Esta funcionalidade reconhece entidades (legendas/rótulos) numa imagem. Por exemplo, na imagem abaixo, ela pode reconhecer as legendas: estádio, desporto, evento, lazer, futebol, rede, planta,etc.</p><figure name="3199" id="3199" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 463px;"><div class="aspectRatioPlaceholder-fill"></div><img class="graf-image" data-image-id="1*78RCU2PiVq5Efkd1yGGxSw.jpeg" data-width="1024" data-height="678" src="https://cdn-images-1.medium.com/max/800/1*78RCU2PiVq5Efkd1yGGxSw.jpeg"></div></figure><p name="ccbd" id="ccbd" class="graf graf--p graf-after--figure">O ML Kit é capaz de reconhecer mais de 400 entidades, incluindo: pessoas(aglomerado de gente, selfie, sorriso), atividades (a dançar, a comer, a surfar), coisas(um carro, um piano, uma receita), animais(pássaros, gatos, cães), plantas(flores, frutas, vegetais) ou até locais (praias, lagos, montanhas).</p><p name="7ae5" id="7ae5" class="graf graf--p graf-after--p">Para começar, temos de adicionar mais uma dependência ao gradle:</p><pre name="29b7" id="29b7" class="graf graf--pre graf-after--p">implementation &#39;com.google.firebase:firebase-ml-vision-image-label-model:15.0.0&#39;</pre><p name="9f5f" id="9f5f" class="graf graf--p graf-after--pre">E o nosso detector é:</p><pre name="f04c" id="f04c" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">FirebaseVisionLabelDetector detector = FirebaseVision.getInstance()<br>    .getVisionLabelDetector();</code></pre><p name="c47f" id="c47f" class="graf graf--p graf-after--pre">Criei uma Activity que usa todos os detetores em simultâneo para poder testá-los. Se você quiser experimentar também, pode encontrar a Activity <a href="https://gist.github.com/rosariopfernandes/603bb860f7b1ccd82759ba8ce36e52b2" data-href="https://gist.github.com/rosariopfernandes/603bb860f7b1ccd82759ba8ce36e52b2" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">neste gist</a>.</p><h3 name="b052" id="b052" class="graf graf--h3 graf-after--p">Custom Models</h3><p name="5349" id="5349" class="graf graf--p graf-after--h3">Quem já tem experiência com Machine Learning pode utilizar os seus próprios modelos em conjunto com o ML Kit. Crie os modelos utilizando o TensorFlow Lite e importe-os na sua aplicação.</p><p name="4776" id="4776" class="graf graf--p graf-after--p graf--trailing">Por enquanto não entrarei em detalhes sobre os custom models, mas se você estiver interessado/curioso, pode saber mais na <a href="https://firebase.google.com/docs/ml-kit/android/use-custom-models?hl=pt-pt" data-href="https://firebase.google.com/docs/ml-kit/android/use-custom-models?hl=pt-pt" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">documentação</a>.</p></div></div></section><section name="5a2e" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="4b8f" id="4b8f" class="graf graf--p graf--leading">Era de se esperar que o Firebase estivesse a preparar uma super novidade para o Google I/O 2018. O ML Kit é certamente bem vindo. De certeza que irá ajudar vários desenvolvedores a criarem aplicações melhores.</p><p name="8682" id="8682" class="graf graf--p graf-after--p">A facilidade de uso é uma das principais vantagens. Apesar da componente Cloud ser paga, a componente grátis (on-device) é bastante poderosa e útil.</p><p name="08d8" id="08d8" class="graf graf--p graf-after--p graf--trailing">Eu sempre quis experimentar a Cloud Vision API, mas o facto dela exigir um cartão de crédito sempre me fez hesitar. Mas agora que existe o ML Kit, posso por em prática todas as ideias que tenho tido para projetos futuros. Irei partilhar algumas delas aqui no Medium, então continue me seguindo para não perdê-las.</p></div></div></section><section name="2f75" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="e2d1" id="e2d1" class="graf graf--p graf--leading graf--trailing">Caso tenha alguma dúvida ou sugestão, deixe abaixo nos comentários. Se você estiver tentando usar o ML Kit e teve um problema, <a href="https://pt.stackoverflow.com/questions/ask?tags=firebase" data-href="https://pt.stackoverflow.com/questions/ask?tags=firebase" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">coloque ele no StackOverflow</a>, explicando o que você fez e qual foi o erro que teve. De certeza que você obterá ajuda de mim ou de alguém da comunidade. 🙂</p></div></div>
